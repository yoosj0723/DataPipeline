{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "\n",
    "os.environ['SPARK_HOME'] = \"/Users/yoosj/Desktop/datapipe/venv1/Analisis/spark-2.4.8-bin-hadoop2.7\"\n",
    "findspark.init()\n",
    "sc = pyspark.SparkContext(appName=\"SparkExample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS에 데이터 로드 후 Spark DF에 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시나리오: airbnb.csv파일을 로드하고 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_path = \"hdfs://localhost;9000/user/hadoop\"\n",
    "file_path = hdfs_path + \"airbnb.csv\"\n",
    "df = sqlContext.read.format(\"csv\")\\\n",
    "    .option(\"header\", 'ture')\\\n",
    "    .option(\"inferSchema\", 'false')\\\n",
    "    .load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "해당 열의 null값 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.na.fill(value=\"{채우고 싶은 값}\", subset={\"해당 열\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 readcsv 시, 전부 type=string으로 불러옴\n",
    "\n",
    "그래서 이것들을 원본의 type에 맞게 전처리해 해줘야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_preprocessing = df.select(\n",
    "    df.{열 이름}.cast({타입}()),\n",
    "    df.{열 이름}.cast({타입}()),\n",
    "    df.{열 이름}.cast({타입}())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_preprocessing.withColumn(\"total_score\", df_preprocessing[\"overall_satisfaction\"] + df_preprocessing[\"reviews\"]*0.378)\n",
    "df1 = df1.select(\"room_id\", \"host_idf\", \"total_score\")\n",
    "df1.sort(\"total_score\", ascending=True).write.csv({저장 경로.csv})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "df2 = df_preprocessing\n",
    "df2 = df2.groupBy(\"neighborhood\")\\\n",
    "    .agg(\n",
    "        F.mean(\"reviews\").alias(\"avg of reviews\"),\\\n",
    "        F.mean(\"overall_satisfaction\").alias(\"avg of overall_satisfaction\"),\\\n",
    "        F.mean(\"price\").alias(\"avg of price\"),\\\n",
    "        F.max(\"reviews\").alias(\"max of reviews\"),\\\n",
    "        F.min(\"reviews\").alias(\"min of reviews\"),\\\n",
    "        F.max(\"price\").alias(\"max of price\"),\\\n",
    "        F.min(\"price\").alias(\"min of price\"),\\\n",
    "    ).sort(\"neighborhood\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bucketing 후 Col 이름 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.sql.funcions import udf\n",
    "\n",
    "df3 = df_preprocessing\n",
    "\n",
    "bucketizer = Bucketizer(splits=[0,100,200,300,400,500,1000,5000], inputCol=\"price\", outputCol=\"buckets\")\n",
    "df3_buck = bucketizer.setHandleInvalid(\"keep\").transform(df3)\n",
    "\n",
    "scope = {0.0: \"0-100\", 1.0: \"100-200\", 2.0: \"200-300\", 3.0: \"300-400\", 4.0: \"4000-500\", 5.0: \"500-1000\", 6.0: \"1000-5000\"}\n",
    "udf_scope = udf(lambda x: scope[x], StringType())\n",
    "df3_buck = df3_buck.witColumn(\"price_range\", udf_scope(\"buckets\"))\n",
    "\n",
    "df3_compl = df3_buck.selelct(\"room_idf\", \"price\", \"buckets\", \"price_range\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grouping 후 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_compl = df3_compl.groupBy(\"buckets\", \"price_range\")\\\n",
    "    .agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_length = df3_compl.select('*', F.size(\"neighborhood\").alias('length'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
