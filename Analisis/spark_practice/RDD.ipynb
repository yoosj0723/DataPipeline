{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9552882f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.17 (default, Nov  2 2024, 13:11:17) \n",
      "[Clang 15.0.0 (clang-1500.3.9.4)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b5b8abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = \"/Users/yoosj/Desktop/datapipe/venv1/Analisis/spark-2.4.8-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3818f850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31baac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/02 15:02:41 WARN Utils: Your hostname, yuseungjaeui-MacBookPro.local resolves to a loopback address: 127.0.0.1; using 192.168.0.12 instead (on interface en0)\n",
      "24/11/02 15:02:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "24/11/02 15:02:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/02 15:02:41 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"SparkExample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef88fb0",
   "metadata": {},
   "source": [
    "# Transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea094749",
   "metadata": {},
   "source": [
    "## map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d735f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['cat', 'cat'], ['ele'], ['cat', 'ele']]\n"
     ]
    }
   ],
   "source": [
    "wordslist = [['cat', 'cat'], ['ele'],['cat', 'ele']]\n",
    "rdd = sc.parallelize(wordslist)\n",
    "output = rdd.map(lambda line: [x for x in line]).collect()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3271a44",
   "metadata": {},
   "source": [
    "## flatMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88bfdb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'cat', 'ele', 'cat', 'ele']\n"
     ]
    }
   ],
   "source": [
    "wordslist = [['cat', 'cat'], ['ele'],['cat', 'ele']]\n",
    "rdd = sc.parallelize(wordslist)\n",
    "output = rdd.flatMap(lambda line: [x for x in line]).collect()\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abbc89",
   "metadata": {},
   "source": [
    "## filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a36423ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ele', 'ele']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_cat(animal):\n",
    "    return(animal != 'cat')\n",
    "\n",
    "wordslist = [['cat', 'cat'], ['ele'],['cat', 'ele']]\n",
    "rdd = sc.parallelize(wordslist)\n",
    "rdd.flatMap(lambda line: [x for x in line]) \\\n",
    "    .filter(remove_cat).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fa67fc",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41297e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ele', 2)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_cat(animal):\n",
    "    return(animal != 'cat')\n",
    "\n",
    "wordslist = [['cat', 'cat'], ['ele'],['cat', 'ele']]\n",
    "rdd = sc.parallelize(wordslist)\n",
    "rdd.flatMap(lambda line: [x for x in line]) \\\n",
    "    .filter(remove_cat) \\\n",
    "    .map(lambda ele: (ele,1)) \\\n",
    "    .reduceByKey(lambda x,y : x + y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e193298",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a848a1",
   "metadata": {},
   "source": [
    "## wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ba6ea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ele', 1), ('rat', 2), ('cat', 2)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsList = ['cat', 'ele', 'rat', 'rat', 'cat']\n",
    "wordsRDD = sc.parallelize(wordsList)\n",
    "\n",
    "output = wordsRDD.map(lambda x: (x,1)) \\\n",
    ".reduceByKey(lambda x,y: x+y)\n",
    "\n",
    "output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a22efa",
   "metadata": {},
   "source": [
    "## 2 \n",
    "\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f01fc25d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Lee loves soccer', 'Yoo loves bread']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = [(0, \"Lee\"), (1, \"Jang\"), (2, \"Yoo\"), (3, \"Park\"), (4, \"Kim\")]\n",
    "subs = [(0, \"soccer\"), (2, \"bread\")]\n",
    "usersRDD = sc.parallelize(users)\n",
    "subsRDD = sc.parallelize(subs)\n",
    "\n",
    "mergedRDD = usersRDD.rightOuterJoin(subsRDD)\n",
    "mergedRDD.map(lambda value : value[1][0] + ' loves ' + value[1][1]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae6efce",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2ad1dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"name\": \"Sparky The Bear\", \"lovePandas\": true}',\n",
       " '{\"name\": \"Sparky The Bear\", \"lovePandas\": true, \"knows\": {\"friends\": [\"holden\"]}}']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def lovePandas(line):\n",
    "    k = 'lovePandas'\n",
    "    if k in line and line[k]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "path = \"./data/\"\n",
    "input_pile = sc.textFile(path + \"pandas.json\")\n",
    "\n",
    "# Json => Python dictionary\n",
    "pandasJson = input_pile.map(lambda x: json.loads(x))\n",
    "\n",
    "# Python dictionary => Json\n",
    "output = pandasJson.filter(lovePandas).map(lambda x: json.dumps(x))\n",
    "output.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_pyspark)",
   "language": "python",
   "name": "venv_pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
